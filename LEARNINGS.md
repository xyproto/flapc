# Flapc Compiler Learnings

## Stack Alignment in x86-64

### The 16-byte Alignment Rule

The x86-64 System V ABI requires the stack pointer (rsp) to be aligned to 16 bytes **before** making any function call. This is critical when calling external functions like malloc, printf, etc.

### How to Calculate Stack Alignment

When a function is called, the CPU automatically pushes the return address (8 bytes). So at function entry, rsp is misaligned by 8 bytes.

Stack layout after various operations:
- After `call`: +8 bytes (misaligned - now at 8-byte boundary)
- After `push rbp`: +8 bytes (aligned - now at 16-byte boundary)
- After each `push`: +8 bytes per register

**Example calculation:**
```
call instruction         : +8  (total: 8,  misaligned)
push rbp (prologue)      : +8  (total: 16, aligned)
push r12                 : +8  (total: 24, misaligned)
push r13                 : +8  (total: 32, aligned)
push r14                 : +8  (total: 40, misaligned)
push r15                 : +8  (total: 48, aligned)
push rbx                 : +8  (total: 56, misaligned)
push rdi                 : +8  (total: 64, aligned)
```

Before calling malloc or any external function, count your stack usage. If it's misaligned (not a multiple of 16), subtract 8 more bytes from rsp.

### The Bug Pattern

In `flap_string_to_cstr` (parser.go line ~7520), we had:

```go
// BUGGY CODE (removed):
fc.out.SubImmFromReg("rsp", StackSlotSize)  // Sub 8
fc.out.MovXmmToMem("xmm0", "rsp", 0)
fc.out.MovMemToReg("r12", "rsp", 0)
fc.out.AddImmToReg("rsp", StackSlotSize)    // BUG: Added back too early!
```

At this point:
- call (8) + 6 pushes (48) = 56 bytes on stack
- 56 is not a multiple of 16 (misaligned!)
- The `sub rsp, 8` made it 64 bytes (aligned)
- But then we added it back before calling malloc
- malloc was called with misaligned stack → segfault or garbage data

**Fix:** Keep the stack aligned through the malloc call:

```go
// FIXED CODE:
fc.out.SubImmFromReg("rsp", StackSlotSize)  // Sub 8, now aligned
fc.out.MovXmmToMem("xmm0", "rsp", 0)
fc.out.MovMemToReg("r12", "rsp", 0)
// Keep rsp subtracted - restored later at line 7659
```

### General Principle

**Always verify stack alignment before calling external functions:**

1. Count bytes on stack: call(8) + pushes(8*N) + local_space
2. If total % 16 ≠ 0, subtract 8 more from rsp
3. Keep stack aligned until after the call returns
4. Restore rsp after the call completes

### Debugging Stack Alignment

If you see segfaults or garbage data from malloc/printf/etc:
1. Check stack alignment before the call
2. Use gdb: `info registers` and check rsp value
3. rsp & 0xF should equal 0 (bottom 4 bits zero)
4. Use ndisasm to verify generated assembly

### Impact

Incorrect stack alignment causes:
- Segmentation faults in external functions
- Garbage/corrupted return values
- Undefined behavior in SSE/AVX instructions (they require alignment)
- Intermittent bugs that are hard to reproduce

## Helper Function for Aligned malloc Calls

To make stack alignment easier and prevent bugs, we created a helper function:

```go
func (fc *FlapCompiler) callMallocAligned(sizeReg string, pushCount int)
```

**Parameters:**
- `sizeReg`: Register containing the allocation size (will be moved to rdi)
- `pushCount`: Number of registers pushed after the function prologue (not including `push rbp`)

**What it does:**
1. Calculates current stack usage: 16 + (8 * pushCount)
2. Checks if alignment is needed (total % 16 != 0)
3. Moves size to rdi (first argument for malloc)
4. Subtracts 8 from rsp if needed for alignment
5. Calls malloc
6. Restores rsp if it was adjusted
7. Returns allocated pointer in rax

**Usage example:**
```go
// Function with 5 register pushes after prologue
fc.out.PushReg("rbx")
fc.out.PushReg("r12")
fc.out.PushReg("r13")
fc.out.PushReg("r14")
fc.out.PushReg("r15")

// Allocate 512 bytes
fc.out.MovImmToReg("rax", "512")
fc.callMallocAligned("rax", 5) // 5 pushes
// Result is in rax
```

This replaces the manual alignment pattern:
```go
// OLD WAY (manual):
fc.out.SubImmFromReg("rsp", StackSlotSize)  // For alignment
fc.out.MovRegToReg("rdi", "rax")
fc.trackFunctionCall("malloc")
fc.eb.GenerateCallInstruction("malloc")
fc.out.AddImmToReg("rsp", StackSlotSize)  // Restore

// NEW WAY (helper):
fc.callMallocAligned("rax", pushCount)
```

The helper automatically handles alignment, making code clearer and preventing mistakes.

## When Stack Alignment Is Needed

### Main Function Context (Already Aligned)

In the main function generated by Flap, the stack is pre-aligned:
```
_start:
  // RSP is 16-byte aligned (kernel guarantee)
  jmp main

main:
  push rbp           // RSP now at (16n - 8)
  mov rbp, rsp
  // No further adjustment needed
```

After `push rbp`, RSP is at (16n - 8). When we make a C function call:
- `call` pushes return address (8 bytes) → RSP becomes 16n (aligned!)
- Function prologue in C function maintains alignment

So **C function calls from the main function don't need manual alignment**.

### Runtime Helper Functions (Need Alignment)

Runtime helpers we generate (like `flap_string_to_cstr`, `flap_cache_insert`, etc.) have their own prologue and often push registers:

```
flap_helper:
  call             // +8 (RSP = 16n - 8)
  push rbp         // +8 (RSP = 16n)
  mov rbp, rsp
  push r12         // +8 (RSP = 16n - 8)
  push r13         // +8 (RSP = 16n)
  push r14         // +8 (RSP = 16n - 8)
  push r15         // +8 (RSP = 16n)
  push rbx         // +8 (RSP = 16n - 8) -- MISALIGNED!

  // Calling malloc here would crash!
```

After an odd number of pushes (after the prologue), RSP is misaligned. We need to:
1. Count the pushes
2. If count is odd, subtract 8 before calling C functions
3. Restore after the call

**This is where `callMallocAligned(sizeReg, pushCount)` is essential.**

### General Rule

- **Main function → C function**: Already aligned, no action needed
- **Runtime helper → C function**: Must use alignment helper or manually align
- **Runtime helper → runtime helper**: Each function handles its own alignment

The helper function automatically calculates: `(16 + 8*pushCount) % 16 != 0`

## Register Clobbering and the Stack-First Principle

### The Problem

Registers are volatile across function calls. Any XMM register (xmm0-xmm15) or general-purpose register can be clobbered when evaluating sub-expressions that contain function calls.

**Example of the bug pattern:**
```go
// BUGGY CODE (removed from binary operations):
fc.compileExpression(e.Left)           // Result in xmm0
fc.out.MovRegToReg("xmm2", "xmm0")     // Save left in xmm2
fc.compileExpression(e.Right)          // May call functions that clobber xmm2!
fc.out.MovRegToReg("xmm0", "xmm2")     // BUG: xmm2 is corrupted!
```

This manifested in expressions like `n * factorial(n - 1)`, where:
1. `n` is evaluated and stored in xmm2
2. `factorial(n - 1)` is evaluated, which recursively uses xmm registers
3. When control returns, xmm2 contains garbage, not `n`
4. The multiplication uses corrupted values

### The Solution: Stack-First Principle

**Always use the stack for intermediate values across sub-expression evaluations:**

```go
// CORRECT CODE (current implementation):
fc.compileExpression(e.Left)           // Result in xmm0
fc.out.SubImmFromReg("rsp", 16)        // Allocate stack space
fc.out.MovXmmToMem("xmm0", "rsp", 0)   // Save left to stack
fc.compileExpression(e.Right)          // Safe - can use any registers
fc.out.MovRegToReg("xmm1", "xmm0")     // Right in xmm1
fc.out.MovMemToXmm("xmm0", "rsp", 0)   // Restore left from stack
fc.out.AddImmToReg("rsp", 16)          // Clean up
// Now perform operation with xmm0 and xmm1
```

### When Registers Are Safe vs. Unsafe

**Safe to use registers:**
- Within a single basic block with no function calls
- For immediate operations (e.g., `movsd xmm1, xmm0` followed by `addsd xmm0, xmm1`)
- For results that are used immediately before any function call

**Must use stack:**
- Across sub-expression evaluations that might contain function calls
- Across loop iterations where the loop body might call functions
- When the value needs to survive a function call

### General Guidelines

1. **Default to stack-based storage** for any value that needs to persist across sub-expression evaluation
2. **Only optimize to registers** when you can prove no function calls intervene
3. **Document assumptions** when using register storage (e.g., "safe because no calls in this basic block")
4. **Use descriptive comments** like "Save to stack (registers may be clobbered by function calls)"

### x86-64 Calling Convention Register Usage

According to System V ABI, these registers are caller-saved (clobbered by function calls):
- **General purpose**: rax, rcx, rdx, rsi, rdi, r8-r11
- **XMM registers**: xmm0-xmm15 (all volatile)

These are callee-saved (preserved across calls):
- **General purpose**: rbx, rbp, r12-r15
- **XMM registers**: None! All XMM registers are caller-saved

**Implication:** XMM registers are NEVER safe across function calls. Always use stack.

### Performance Considerations

Stack operations are fast (L1 cache) and the slight overhead is negligible compared to:
- The complexity of register liveness analysis
- The difficulty of debugging register corruption bugs
- The risk of subtle, hard-to-reproduce errors

**Premature optimization**: Trying to "optimize" by using registers for intermediate values often leads to bugs that cost far more time to debug than the microseconds saved.

### Code Patterns and Helpers

**Helper function for safe binary operations:**

```go
// Use this helper instead of manually managing registers:
func (fc *FlapCompiler) compileBinaryOpSafe(left, right Expression, operator string)
```

This helper encapsulates the stack-first pattern and should be used whenever possible.

**Comment template for manual implementations:**

When you must manually implement expression compilation with sub-expressions, use this comment pattern:

```go
// Compile left operand
fc.compileExpression(leftExpr)
// Save to stack (registers may be clobbered by sub-expression evaluation)
fc.out.SubImmFromReg("rsp", 16)
fc.out.MovXmmToMem("xmm0", "rsp", 0)
// Compile right operand (safe - can use any registers)
fc.compileExpression(rightExpr)
// Restore left operand from stack
fc.out.MovMemToXmm("xmm1", "rsp", 0)
fc.out.AddImmToReg("rsp", 16)
// Now xmm0 has right, xmm1 has left - ready to use
```

**Red flags to watch for:**

These patterns are potential bugs:
- `fc.out.MovRegToReg("xmm2", "xmm0")` followed by `fc.compileExpression(...)` - xmm2 will likely be clobbered
- Saving to XMM registers (xmm2-xmm15) across `fc.compileExpression()` calls
- Assuming any XMM register preserves its value across function calls
- Using XMM registers for "temporary" storage without checking call paths

**Safe patterns:**

These patterns are safe:
- Stack-based storage: `SubImmFromReg` → `MovXmmToMem` → ... → `MovMemToXmm` → `AddImmToReg`
- Using callee-saved general-purpose registers (rbx, r12-r15) but ONLY in functions you control the prologue/epilogue for
- Register-to-register moves within a single basic block with no function calls

## Nested Loop Implementation Design

**Problem:** Supporting arbitrary depth nested loops where each loop maintains its counter, limit, and iterator variable.

**Failed Approach:** Register-based storage using r12/r13
- Works for 2 levels but fails for 3+ because only saves the immediately outer loop's registers
- Push/pop pattern creates LIFO order: push A's registers → push B's registers → pop B's registers → pop A's registers
- Inner loop restore happens before outer loop completes

**Correct Solution:** Stack-based storage
```
Each loop allocates dedicated stack space (32 bytes, 16-byte aligned):
- [rsp + 0]:  counter (current iteration value)
- [rsp + 8]:  limit (end value)  
- [rsp + 16]: (padding for alignment)

Loop execution:
1. Allocate stack space: sub rsp, 32
2. Store counter/limit to stack
3. Load counter/limit to r12/r13 for loop condition checks
4. Update counter, store back to stack
5. Nested loops allocate their own stack slots
6. Deallocate on exit: add rsp, 32
```

**Key insight:** Each nested loop level has isolated stack slots, preventing interference.

**Files:** `parser.go:4419-4516` (compileRangeLoop function)

## Stack Alignment and Printf Bug

**Problem:** SIGBUS crashes when calling printf after nested loops.

**Root Cause #1:** Range loops allocated 24 bytes (not 16-byte aligned), violating x86-64 ABI requirement.

**Root Cause #2:** Printf had buggy alignment code:
```go
// WRONG - r10 is caller-saved, gets clobbered by printf
fc.out.MovRegToReg("r10", "rsp")
fc.out.AndImm("rsp", -16)
// ... call printf ...
fc.out.MovRegToReg("rsp", "r10")  // BROKEN: r10 was clobbered!
```

**Solution:**
1. Changed range loop allocation from 24 to 32 bytes (16-byte aligned)
2. Removed buggy printf alignment code - no longer needed since stack is always aligned

**Lesson:** Stack must be 16-byte aligned before any function call. Use proper multiples (16, 32, 48, ...) for stack allocations.

## Variable Scoping and Priority Order in Optimization Passes

When resolving variable/parameter references during optimization, the priority order is:

1. **Lambda parameters** - Highest priority, shadows all outer scopes
2. **Loop iterators** - Local to loop scope
3. **Local variables** - Variables defined in the current scope
4. **Outer scope variables** - Variables from enclosing scopes
5. **Constants** - Constant propagation applies last, only if not shadowed

### Constant Propagation and Lambda Scoping

**Critical Rule:** Lambda parameters must shadow outer variables during constant propagation.

**Bug Pattern:**
```flap
x := 10.5              // Outer variable marked as constant
square := x => x * x   // Lambda parameter 'x'
square(4.0)            // WRONG: returns 110.25 (10.5 * 10.5)
                       // RIGHT: should return 16 (4.0 * 4.0)
```

**Cause:** Constant propagation replaced lambda parameter `x` with outer constant `10.5`.

**Solution:** When propagating into lambda bodies, temporarily remove lambda parameters from the constants map:

```go
case *LambdaExpr:
    savedConstants := make(map[string]Expression)
    for _, param := range e.Params {
        if oldVal, existed := cp.constants[param]; existed {
            savedConstants[param] = oldVal
            delete(cp.constants, param)
        }
    }

    // Propagate into body with parameters shadowing outer constants
    if newBody, bodyChanged := cp.propagateInExpr(e.Body); bodyChanged {
        e.Body = newBody
    }

    // Restore outer constants
    for param, oldVal := range savedConstants {
        cp.constants[param] = oldVal
    }
```

### Mutation Tracking in Expressions

Constant propagation must detect mutations that occur within expressions, not just in assignment statements.

**Mutations can occur in:**
- Match expression branches: `n % 2 == 0 { -> n <- n / 2 }`
- Block expressions
- Lambda bodies
- Binary expressions with `<-` operator
- Postfix expressions: `steps++`

**Implementation:** Add `findMutationsInExprWithDepth()` that recursively searches expressions with depth limiting (max 100 levels) to prevent infinite recursion.

**Example requiring mutation tracking:**
```flap
n := 27
n % 2 == 0 {
    -> n <- n / 2      // Mutation in match branch
    ~> n <- (3*n) + 1  // Must be detected
}
```

### Dead Code Elimination Expression Handling

DCE must track variable usage in all expression types:

**Critical expression types:**
- `FStringExpr` - F-string interpolations: `f"Hello {name}"`
- `DirectCallExpr` - Direct function calls
- `NamespacedIdentExpr` - Dot notation: `data.field`
- `PostfixExpr` - Postfix operations: `i++`
- `VectorExpr` - Vector literals
- `LoopExpr` - Loop expressions
- `MultiLambdaExpr` - Pattern matching lambdas

**Bug Pattern:** Variable marked as unused and removed, causing "undefined variable" errors.

**Solution:** Add cases in `markUsedInExpr()` for all expression types that can reference variables.

### Loop Unrolling State Expression Handling

When unrolling loops with loop state expressions (`@i`, `@i1`, `@i2`):

**Loop Level Semantics:**
- `@i` (LoopLevel=0) - Current loop iterator
- `@i1` (LoopLevel=1) - Outermost loop iterator
- `@i2` (LoopLevel=2) - Second level loop iterator
- etc.

**Unrolling Rules:**
1. Only unroll loops with constant bounds and ≤ 8 iterations
2. Check if loop contains nested loops before substitution
3. When unrolling:
   - Replace `@i1` (LoopLevel=1) with iteration value
   - Decrement LoopLevel for `@i2+` (LoopLevel>1)
   - Only replace `@i` (LoopLevel=0) if no nested loops

**Example:**
```flap
@ i in 0..<3 {              // Outer loop
    @ j in 10..<12 {         // Inner loop
        printf("@i1=%v, @i2=%v, @i=%v", @i1, @i2, @i)
    }
}
```

After outer loop unrolls:
- `@i1` → 0, 1, 2 (replaced with values)
- `@i2` → `@i1` (LoopLevel decremented from 2 to 1)
- `@i` → `@i` (stays as-is, will be replaced when inner loop unrolls)

### Recursion Safety

All recursive AST traversals must include depth limiting to prevent stack overflow on malformed or adversarial input.

**Implementation Pattern:**
```go
const maxRecursionDepth = 100

func traverse(node Node, depth int) {
    if depth > maxRecursionDepth {
        return  // Or return error
    }
    // Process node...
    traverse(child, depth+1)
}
```

**Apply to:** findMutations, propagateInExpr, markUsedInExpr, and any other recursive AST traversal.

## macOS ARM64 Execution Issue (2025-10-26)

### Discovery
macOS ARM64 binaries generated by flapc are structurally valid Mach-O files but are killed with SIGKILL (exit code 137) before code execution. Even `codesign` reports "failed strict validation".

### Progress
1. Fixed LINKEDIT segment size calculation - was including 4KB code signature space when none was written
2. Added LC_CODE_SIGNATURE load command with 4KB reserved space (zeros)
3. Binary now has proper structure with filesize matching actual data

### Fixed Issues (Session 2)
1. __LINKEDIT segment now always generated on macOS (not just for dynamic linking)
2. LC_SYMTAB always written (required for code signature)
3. LC_UUID load command added (dyld requirement)
4. LC_BUILD_VERSION updated to match system version (26.0 for Sequoia)
5. LINKEDIT structures properly sized and written

### Current Status
**Simple binaries (no function calls):**
- Binary structure is valid per `otool`
- `codesign` completes without error but reports "no signature" after
- Execution: SIGKILL (137) - macOS kills unsigned binaries
- Issue: codesign can't sign our binaries for unknown reason

**Binaries with dynamic linking:**
- When MH_DYLDLINK flag set, binary loads but crashes with SIGSEGV (139)
- dyld error: tries to read relocation data but derefs NULL pointer
- Crash in `dyld::forEachRebase_Relocations` at address 0x48
- Issue: Dynamic linking enabled but no relocation structures provided

### Key Findings
1. macOS 26 (Sequoia) has stricter requirements than macOS 12
2. Even simple binaries from Clang link to libSystem.B.dylib
3. Clang uses LC_DYLD_CHAINED_FIXUPS (we removed this for lazy binding)
4. When dynamic linking enabled, dyld expects valid relocation data
5. codesign tool can't create signature - binary structure issue?

### Fixed Issues (Session 3) - Dynamic Linking with GOT/Stubs

**Root Cause:** Two critical issues prevented dynamic linking from working:

1. **Incorrect LINKEDIT section order**
   - We wrote: `symtab → strtab → indirect symtab → code signature`
   - Apple expects: `symtab → indirect symtab → strtab → code signature`
   - When `ldid` signed the binary, it assumed Apple's order and overwrote our indirect symbol table with zeros
   - Result: GOT/stub entries pointed to wrong symbols, causing printf to fail

2. **Missing two-level namespace library ordinal**
   - Symbol table N_desc field was set to 0 for undefined symbols
   - Should be: `(library_ordinal << 8)` where libSystem.B.dylib = ordinal 1
   - Result: dyld couldn't find which library provides printf
   - Error: "Symbol not found: _printf, Expected in: <binary>" instead of "Expected in: libSystem"

**The Fix:**

1. Reordered LINKEDIT sections to match Apple's expected layout:
```go
// Correct order in WriteMachO():
// 1. Write symbol table
// 2. Write indirect symbol table (for dynamic linking)
// 3. Write string table
// 4. Reserve code signature space
```

2. Set correct N_desc for undefined symbols:
```go
sym := Nlist64{
    N_strx:  strOffset,
    N_type:  N_UNDF | N_EXT,
    N_sect:  0,
    N_desc:  uint16(1 << 8), // Dylib ordinal 1 (libSystem.B.dylib)
    N_value: 0,
}
```

**Result:** Dynamic linking now works! Binaries using printf execute correctly with exit code 0.

**Files Modified:**
- `macho.go` lines 955-1013: Updated offset calculations for new LINKEDIT order
- `macho.go` lines 1176-1193: Reordered actual writing of LINKEDIT sections
- `macho.go` line 637: Set N_desc with library ordinal for two-level namespace

**Validation:**
```bash
$ ./flapc testprograms/const_test.flap
$ ./const_test
0
$ echo $?
0
```

**Key Learning:** The order of sections in LINKEDIT matters! Tools like `ldid` make assumptions about the layout and will corrupt data if the order doesn't match Apple's conventions. Always check:
1. What offset calculations assume
2. What order data is actually written
3. What external tools (like ldid) expect

---

## Network Send Operator: Evolution from `<-` to `<==` (2025-01-27)

**Problem 1:** Initial ENet design used `<-` for network send (`:5000 <- "message"`), but this created ambiguity with variable updates (`x <- x + 1`). When the left-hand side is a variable holding a port number, parser cannot distinguish:
```flap
port := :5000
port <- "message"  // Send? Or update variable?
```

**Solution 1:** Use `<=` operator for network send operations.

**Problem 2:** Using `<=` for sends created confusion with the comparison operator (`x <= 10`). While technically unambiguous to the parser, it's confusing for developers reading code.

**Final Solution:** Use `<==` operator for network send operations.

**Rationale:**
- **No ambiguity**: `<==` is distinct from both `<-` (variable update) and `<=` (comparison)
- **Visually intuitive**: Three characters suggest "send toward" or "push to"
- **No operator overloading**: Dedicated operator for dedicated purpose
- **Parser simplicity**: Dedicated token (TOKEN_SEND) with no dual purposes

**Implementation:**
```flap
// Variable update
x <- x + 1

// Comparison
if x <= 10 { }

// Network send
:5000 <== "hello"                // Send to port
port <== "message"               // Send to variable containing port
"server.com:5000" <== "data"     // Send to remote address
```

**Files Modified:**
- `lexer.go`: Added TOKEN_SEND for `<==`
- `ast.go`: SendExpr uses `<==` in String()
- `parser.go`: parseSend() checks TOKEN_SEND instead of TOKEN_LE
- `LANGUAGE.md`: Updated all examples and grammar rules

**Key Learning:** When overloading operators, choose combinations that minimize ambiguity. If context-based resolution requires complex lookahead, consider using a different operator entirely.
## Port Addresses: Strings vs. Special Literals (2025-01-27)

**Problem:** Initial design used special port literal syntax (`:5000`, `:worker`) with TOKEN_PORT and PortExpr AST nodes. This required:
- Special lexer handling with bracket depth tracking
- Dedicated AST node type
- Compile-time hashing for named ports
- Complex parsing to avoid conflicts with slice syntax `[0:2]`

**Solution:** Use regular strings for port addresses: `":5000"`, `"localhost:5000"`

**Rationale:**
- **Simpler parser**: No special token or AST node needed
- **Familiar syntax**: Strings are already well-understood
- **Flexible format**: Easy to extend to "host:port" format
- **Less ambiguity**: No conflict with colon in slices/maps
- **Runtime flexibility**: Can support variables holding addresses (future enhancement)

**Implementation:**
```flap
// Before (special syntax)
:5000 <== "hello"
:worker <== "data"

// After (strings)
":5000" <== "hello"
":8080" <== "data"
"server.com:5000" <== "remote"
```

**Files Modified:**
- `lexer.go`: Removed TOKEN_PORT, readPortLiteral(), bracket depth tracking
- `ast.go`: Removed PortExpr type
- `parser.go`: Removed portToNumber(), simplified compileSendExpr to parse string literals
- `LANGUAGE.md`: Updated all examples to use strings

**Key Learning:** Sometimes the simplest solution is to reuse existing language features rather than adding special syntax. Strings are flexible and well-understood - no need for custom literals.

## Futex Barriers and Parallel Loop Synchronization

### The Challenge: Thread Synchronization Without Pthreads

When implementing parallel loops (`@@` and `N @`), we needed a way to synchronize threads without linking to pthread. The goal: spawn threads via raw `clone()` syscalls and coordinate completion using only kernel primitives.

### Atomic Operations: The Foundation

**Learning 1: LOCK XADD is your friend for atomic decrements**

The key insight: futex barriers need atomic counter operations. The x86-64 `LOCK XADD` instruction is perfect for this:

```asm
mov eax, -1
lock xadd [barrier_addr], eax  ; Atomically: tmp=mem; mem+=eax; eax=tmp
dec eax                         ; eax now has new value
test eax, eax                   ; Check if we're last thread
```

**Why LOCK XADD over LOCK DEC?**
- LOCK XADD returns the OLD value, letting us know the NEW value after decrement
- LOCK DEC doesn't return any value, only sets flags (harder to use)
- Pattern: `old = atomic_add(ptr, -1); new = old - 1; if (new == 0) { last_thread(); }`

**Implementation in atomic.go:**
- Emits proper REX prefix for 64-bit registers
- Uses ModR/M encoding for memory operands with displacements
- Supports x86-64 (LOCK XADD), ARM64 (LDADD), RISC-V (AMO instructions)

### Futex Syscalls: Linux Fast Userspace Mutex

**Learning 2: FUTEX_PRIVATE_FLAG gives you free performance**

Futex operation codes:
```go
FUTEX_WAIT = 0          // Block until woken
FUTEX_WAKE = 1          // Wake N threads
FUTEX_PRIVATE_FLAG = 128 // Don't share across processes
```

Always use the PRIVATE variant for thread-only synchronization:
```go
FUTEX_WAIT_PRIVATE = 128  // 0 | 128
FUTEX_WAKE_PRIVATE = 129  // 1 | 128
```

The PRIVATE flag tells the kernel this futex won't be shared across processes, enabling optimizations:
- No need to hash into a global kernel table
- Faster lookup (process-local table only)
- ~10-20% better performance vs non-private futex

### Barrier Pattern: N-Thread Rendezvous

**Learning 3: Initialize counter to N, parent waits too**

Initial attempt (WRONG):
```go
// BUG: Only child decrements, parent never woken
barrier.count = 1
// Child: atomic_dec(count); if (count == 0) wake_parent();
// Parent: wait_on_futex(count);
// Problem: Parent waits on value 1, child sets it to 0 and wakes,
//          but if parent hasn't started waiting yet, wake is lost!
```

Correct pattern:
```go
// All threads participate in the barrier
barrier.count = num_threads + 1  // +1 for parent

// Child threads:
old = atomic_add(&barrier.count, -1)
if (old - 1 == 0) {  // Last one out
    futex(&barrier.count, FUTEX_WAKE_PRIVATE, barrier.total)
} else {
    futex(&barrier.count, FUTEX_WAIT_PRIVATE, expected_value)
}

// Parent:
futex(&barrier.count, FUTEX_WAIT_PRIVATE, current_value)
// Wakes when count reaches 0
```

**Current V4 Implementation:**
For simplicity, V4 spawns 1 child thread and parent waits:
```go
barrier.count = 1  // Only child participates
// Child decrements, wakes parent
// Parent waits until woken
```

This works but is limited. V5 will spawn N children and all will coordinate via the barrier.

### Memory Layout: Passing Data to Threads

**Learning 4: Store arguments on child stack BEFORE clone()**

The child thread needs to know:
- Work range: [start, end)
- Barrier address for synchronization

Solution: Write to child's stack before it starts:
```go
// Allocate 1MB stack
stack_top = mmap(NULL, 1MB, PROT_READ|PROT_WRITE, ...)

// Store arguments at negative offsets from stack top
[stack_top - 24] = start       // 8 bytes
[stack_top - 16] = end         // 8 bytes
[stack_top - 8]  = barrier_ptr // 8 bytes

// Adjust stack pointer
child_stack = stack_top - 24

// Clone with this stack
clone(CLONE_VM|..., child_stack, ...)
```

Child reads them back:
```asm
mov rbp, rsp           ; Set up frame pointer
mov r12, [rbp+0]       ; r12 = start
mov r13, [rbp+8]       ; r13 = end
mov r15, [rbp+16]      ; r15 = barrier_addr
```

### Clone Flags: Minimal Sharing for Threads

**Learning 5: CLONE_VM is mandatory, CLONE_THREAD is optional**

Required flags for threads:
```go
CLONE_VM        = 0x00000100  // Share memory space
CLONE_FS        = 0x00000200  // Share filesystem info
CLONE_FILES     = 0x00000400  // Share file descriptor table
CLONE_SIGHAND   = 0x00000800  // Share signal handlers
CLONE_SYSVSEM   = 0x00040000  // Share SysV semaphores
```

Optional but useful:
```go
CLONE_THREAD    = 0x00010000  // Thread group (same TGID)
```

Without CLONE_THREAD, each clone gets its own process ID but still shares memory. This is fine for our use case and simpler than managing thread groups.

### Debugging Parallel Code

**Learning 6: Use strace -f to trace all threads**

Essential for debugging:
```bash
# See all syscalls from parent and children
strace -f -e trace=clone,futex,exit ./program

# Output shows thread coordination:
clone(...) = 12345
[pid 12344] futex(..., FUTEX_WAIT_PRIVATE, 1) = <blocks>
[pid 12345] futex(..., FUTEX_WAKE_PRIVATE, 1) = 1
[pid 12344] <resumed>) = 0
```

The `<resumed>` line shows when a blocked syscall continues after being woken.

### Performance Considerations

**Learning 7: Thread overhead ~50μs per spawn**

Measured costs:
- Thread creation (mmap + clone): ~50μs
- Context switch: ~3μs
- Futex wake: ~1μs

**Recommendation:** Only parallelize loops with >1000 iterations or expensive bodies.

For a loop with 100 iterations:
- Sequential: 100 × 10μs = 1ms
- Parallel (2 threads): 2 × 50μs (spawn) + 50 × 10μs + 1μs (futex) = 601μs
- Speedup: 1.66× (not 2×) due to overhead

For a loop with 10000 iterations:
- Sequential: 100ms
- Parallel (2 threads): 0.1ms + 50ms + 0.001ms ≈ 50ms
- Speedup: 2× (overhead negligible)

### Key Insights Summary

1. **LOCK XADD over LOCK DEC**: Returns old value, enabling atomic decrement pattern
2. **FUTEX_PRIVATE_FLAG**: Always use for thread-local synchronization (10-20% faster)
3. **Barrier initialization**: Start with N threads, all participate in countdown
4. **Pass data via stack**: Store arguments on child stack before clone()
5. **Minimal clone flags**: CLONE_VM + CLONE_FS + CLONE_FILES is sufficient
6. **strace -f for debugging**: Essential for understanding multi-threaded execution
7. **Overhead analysis**: Thread spawn costs ~50μs, only profitable for heavy loops

### Files Created

- `atomic.go` (87 lines): LOCK XADD instruction for x86-64/ARM64/RISC-V
- `dec.go` (115 lines): DEC instruction for all architectures
- `parser.go` modifications: compileParallelRangeLoop() with V4 futex barriers

### References

- Linux futex(2) man page: https://man7.org/linux/man-pages/man2/futex.2.html
- Intel x86-64 LOCK prefix: Volume 2A, Section 2.1.2
- pthread_barrier_wait implementation in glibc: nptl/pthread_barrier_wait.c
- Go runtime scheduler: runtime/proc.go (similar barrier patterns)

## Why V5 (Full Loop Body Execution) Is Complex

### The Challenge: Separate Stacks Mean Separate Contexts

After successfully implementing V4 (futex barriers working), I attempted V5: executing the actual loop body statements (like `printf("Loop: %v\n", i)`) instead of just printing ASCII digits.

**V5 crashed immediately with SIGSEGV.**

### The Root Problem

Child threads created with `clone()` have their own separate stacks. This creates fundamental architectural challenges:

**What Doesn't Work:**
```go
// V5 attempt (FAILS):
fc.variables[stmt.Iterator] = iteratorOffset  // Register iterator
for _, s := range stmt.Body {
    fc.compileStatement(s)  // Compile loop body
}
```

**Why It Fails:**

1. **Stack-relative addressing breaks**: Parent's local variables are at offsets from parent's `rbp`. Child's `rbp` points to child's stack, so all offsets are wrong.

2. **Function calls need full runtime**: `printf()` and other builtins expect:
   - Proper stack frame setup
   - Correct calling conventions
   - Access to global data structures
   - String constants at correct addresses

3. **Variable access fails**: Loop body may reference parent's variables (e.g., `x <- x + 1`), but those are on parent's stack, inaccessible to child.

### What V4 Does Right

V4 works because it only does simple, self-contained operations:
```asm
; V4: Print ASCII digit (self-contained, no function calls)
mov rax, r14
add rax, 48        ; Convert to ASCII
mov [rsp], rax
mov rax, 1         ; sys_write
mov rdi, 1         ; stdout
mov rsi, rsp       ; buffer
mov rdx, 2         ; length
syscall            ; Direct syscall, no stack frame needed
```

No variables, no function calls, no stack frame dependencies. Just registers and syscalls.

### What V5 Would Actually Require

To support arbitrary loop body execution in child threads, we need:

**1. Shared Memory Arena**
```go
// Allocate shared memory for loop-accessible variables
arena := mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_SHARED|MAP_ANONYMOUS)

// Store loop-local variables in shared arena, not on stack
iterator_ptr = arena + 0
temp_var_ptr = arena + 8
...
```

**2. Thread-Safe Built-ins**
- Reimplement `printf` to work from child thread context
- Or: use message passing to parent thread
- Or: use lock-protected shared stdio

**3. Position-Independent Code**
- All addresses must be absolute or RIP-relative
- No rbp-relative addressing for cross-thread data
- Function pointers must be globally accessible

**4. Proper Call Frame Setup**
```asm
; Child needs full function prologue
push rbp
mov rbp, rsp
sub rsp, <frame_size>
; Align stack to 16 bytes for calls
; Set up shadow space (Windows) or red zone (Linux)
```

**5. Variable Remapping**
```go
// Map parent's stack variables to shared memory locations
parentVars := fc.variables  // Save
fc.variables = make(map[string]int)

// Remap to shared memory offsets
fc.variables[stmt.Iterator] = sharedMemOffset(0)
// Other variables would need similar remapping
```

### Complexity Estimation

Implementing full V5 properly would require:
- **Shared memory allocator**: 50-100 lines
- **Thread-safe printf**: 100-200 lines or message queue
- **Variable remapping logic**: 50-100 lines
- **Call frame management**: 30-50 lines
- **Position-independent addressing**: Changes throughout codebase

**Total:** ~300-500 lines + architectural changes

### Current V4 Status

**What Works:**
- Thread spawning with mmap + clone()
- Futex barrier synchronization
- Work distribution across threads
- Simple self-contained operations in loops

**What Doesn't Work:**
- Function calls from child threads
- Accessing parent's local variables
- Complex loop bodies with string formatting
- Cross-thread shared state

### Recommended Path Forward

**Option A: Stay with V4**
- Document current limitations
- V4 is still useful for embarrassingly parallel workloads
- Simple operations (math, array updates) work fine

**Option B: Implement V6 (Multiple Threads) First**
- Spawn N threads instead of 1
- Each gets work range
- Still simple operations only
- Validates parallel performance

**Option C: Full V5 Later**
- Requires shared memory infrastructure
- Significant architectural changes
- Better done after V6 proves performance benefits

### Key Learning

**Don't underestimate separate stack complexity.** What seems like "just compile the loop body" actually requires:
- Shared memory management
- Thread-safe runtime functions
- Position-independent addressing
- Proper ABI compliance

V4's simple approach (direct syscalls, no shared state) is elegant precisely because it avoids these issues.


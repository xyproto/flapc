# Flapc Compiler Learnings

## Stack Alignment in x86-64

### The 16-byte Alignment Rule

The x86-64 System V ABI requires the stack pointer (rsp) to be aligned to 16 bytes **before** making any function call. This is critical when calling external functions like malloc, printf, etc.

### How to Calculate Stack Alignment

When a function is called, the CPU automatically pushes the return address (8 bytes). So at function entry, rsp is misaligned by 8 bytes.

Stack layout after various operations:
- After `call`: +8 bytes (misaligned - now at 8-byte boundary)
- After `push rbp`: +8 bytes (aligned - now at 16-byte boundary)
- After each `push`: +8 bytes per register

**Example calculation:**
```
call instruction         : +8  (total: 8,  misaligned)
push rbp (prologue)      : +8  (total: 16, aligned)
push r12                 : +8  (total: 24, misaligned)
push r13                 : +8  (total: 32, aligned)
push r14                 : +8  (total: 40, misaligned)
push r15                 : +8  (total: 48, aligned)
push rbx                 : +8  (total: 56, misaligned)
push rdi                 : +8  (total: 64, aligned)
```

Before calling malloc or any external function, count your stack usage. If it's misaligned (not a multiple of 16), subtract 8 more bytes from rsp.

### The Bug Pattern

In `flap_string_to_cstr` (parser.go line ~7520), we had:

```go
// BUGGY CODE (removed):
fc.out.SubImmFromReg("rsp", StackSlotSize)  // Sub 8
fc.out.MovXmmToMem("xmm0", "rsp", 0)
fc.out.MovMemToReg("r12", "rsp", 0)
fc.out.AddImmToReg("rsp", StackSlotSize)    // BUG: Added back too early!
```

At this point:
- call (8) + 6 pushes (48) = 56 bytes on stack
- 56 is not a multiple of 16 (misaligned!)
- The `sub rsp, 8` made it 64 bytes (aligned)
- But then we added it back before calling malloc
- malloc was called with misaligned stack → segfault or garbage data

**Fix:** Keep the stack aligned through the malloc call:

```go
// FIXED CODE:
fc.out.SubImmFromReg("rsp", StackSlotSize)  // Sub 8, now aligned
fc.out.MovXmmToMem("xmm0", "rsp", 0)
fc.out.MovMemToReg("r12", "rsp", 0)
// Keep rsp subtracted - restored later at line 7659
```

### General Principle

**Always verify stack alignment before calling external functions:**

1. Count bytes on stack: call(8) + pushes(8*N) + local_space
2. If total % 16 ≠ 0, subtract 8 more from rsp
3. Keep stack aligned until after the call returns
4. Restore rsp after the call completes

### Debugging Stack Alignment

If you see segfaults or garbage data from malloc/printf/etc:
1. Check stack alignment before the call
2. Use gdb: `info registers` and check rsp value
3. rsp & 0xF should equal 0 (bottom 4 bits zero)
4. Use ndisasm to verify generated assembly

### Impact

Incorrect stack alignment causes:
- Segmentation faults in external functions
- Garbage/corrupted return values
- Undefined behavior in SSE/AVX instructions (they require alignment)
- Intermittent bugs that are hard to reproduce

## Helper Function for Aligned malloc Calls

To make stack alignment easier and prevent bugs, we created a helper function:

```go
func (fc *FlapCompiler) callMallocAligned(sizeReg string, pushCount int)
```

**Parameters:**
- `sizeReg`: Register containing the allocation size (will be moved to rdi)
- `pushCount`: Number of registers pushed after the function prologue (not including `push rbp`)

**What it does:**
1. Calculates current stack usage: 16 + (8 * pushCount)
2. Checks if alignment is needed (total % 16 != 0)
3. Moves size to rdi (first argument for malloc)
4. Subtracts 8 from rsp if needed for alignment
5. Calls malloc
6. Restores rsp if it was adjusted
7. Returns allocated pointer in rax

**Usage example:**
```go
// Function with 5 register pushes after prologue
fc.out.PushReg("rbx")
fc.out.PushReg("r12")
fc.out.PushReg("r13")
fc.out.PushReg("r14")
fc.out.PushReg("r15")

// Allocate 512 bytes
fc.out.MovImmToReg("rax", "512")
fc.callMallocAligned("rax", 5) // 5 pushes
// Result is in rax
```

This replaces the manual alignment pattern:
```go
// OLD WAY (manual):
fc.out.SubImmFromReg("rsp", StackSlotSize)  // For alignment
fc.out.MovRegToReg("rdi", "rax")
fc.trackFunctionCall("malloc")
fc.eb.GenerateCallInstruction("malloc")
fc.out.AddImmToReg("rsp", StackSlotSize)  // Restore

// NEW WAY (helper):
fc.callMallocAligned("rax", pushCount)
```

The helper automatically handles alignment, making code clearer and preventing mistakes.

## When Stack Alignment Is Needed

### Main Function Context (Already Aligned)

In the main function generated by Flap, the stack is pre-aligned:
```
_start:
  // RSP is 16-byte aligned (kernel guarantee)
  jmp main

main:
  push rbp           // RSP now at (16n - 8)
  mov rbp, rsp
  // No further adjustment needed
```

After `push rbp`, RSP is at (16n - 8). When we make a C function call:
- `call` pushes return address (8 bytes) → RSP becomes 16n (aligned!)
- Function prologue in C function maintains alignment

So **C function calls from the main function don't need manual alignment**.

### Runtime Helper Functions (Need Alignment)

Runtime helpers we generate (like `flap_string_to_cstr`, `flap_cache_insert`, etc.) have their own prologue and often push registers:

```
flap_helper:
  call             // +8 (RSP = 16n - 8)
  push rbp         // +8 (RSP = 16n)
  mov rbp, rsp
  push r12         // +8 (RSP = 16n - 8)
  push r13         // +8 (RSP = 16n)
  push r14         // +8 (RSP = 16n - 8)
  push r15         // +8 (RSP = 16n)
  push rbx         // +8 (RSP = 16n - 8) -- MISALIGNED!

  // Calling malloc here would crash!
```

After an odd number of pushes (after the prologue), RSP is misaligned. We need to:
1. Count the pushes
2. If count is odd, subtract 8 before calling C functions
3. Restore after the call

**This is where `callMallocAligned(sizeReg, pushCount)` is essential.**

### General Rule

- **Main function → C function**: Already aligned, no action needed
- **Runtime helper → C function**: Must use alignment helper or manually align
- **Runtime helper → runtime helper**: Each function handles its own alignment

The helper function automatically calculates: `(16 + 8*pushCount) % 16 != 0`

## Register Clobbering and the Stack-First Principle

### The Problem

Registers are volatile across function calls. Any XMM register (xmm0-xmm15) or general-purpose register can be clobbered when evaluating sub-expressions that contain function calls.

**Example of the bug pattern:**
```go
// BUGGY CODE (removed from binary operations):
fc.compileExpression(e.Left)           // Result in xmm0
fc.out.MovRegToReg("xmm2", "xmm0")     // Save left in xmm2
fc.compileExpression(e.Right)          // May call functions that clobber xmm2!
fc.out.MovRegToReg("xmm0", "xmm2")     // BUG: xmm2 is corrupted!
```

This manifested in expressions like `n * factorial(n - 1)`, where:
1. `n` is evaluated and stored in xmm2
2. `factorial(n - 1)` is evaluated, which recursively uses xmm registers
3. When control returns, xmm2 contains garbage, not `n`
4. The multiplication uses corrupted values

### The Solution: Stack-First Principle

**Always use the stack for intermediate values across sub-expression evaluations:**

```go
// CORRECT CODE (current implementation):
fc.compileExpression(e.Left)           // Result in xmm0
fc.out.SubImmFromReg("rsp", 16)        // Allocate stack space
fc.out.MovXmmToMem("xmm0", "rsp", 0)   // Save left to stack
fc.compileExpression(e.Right)          // Safe - can use any registers
fc.out.MovRegToReg("xmm1", "xmm0")     // Right in xmm1
fc.out.MovMemToXmm("xmm0", "rsp", 0)   // Restore left from stack
fc.out.AddImmToReg("rsp", 16)          // Clean up
// Now perform operation with xmm0 and xmm1
```

### When Registers Are Safe vs. Unsafe

**Safe to use registers:**
- Within a single basic block with no function calls
- For immediate operations (e.g., `movsd xmm1, xmm0` followed by `addsd xmm0, xmm1`)
- For results that are used immediately before any function call

**Must use stack:**
- Across sub-expression evaluations that might contain function calls
- Across loop iterations where the loop body might call functions
- When the value needs to survive a function call

### General Guidelines

1. **Default to stack-based storage** for any value that needs to persist across sub-expression evaluation
2. **Only optimize to registers** when you can prove no function calls intervene
3. **Document assumptions** when using register storage (e.g., "safe because no calls in this basic block")
4. **Use descriptive comments** like "Save to stack (registers may be clobbered by function calls)"

### x86-64 Calling Convention Register Usage

According to System V ABI, these registers are caller-saved (clobbered by function calls):
- **General purpose**: rax, rcx, rdx, rsi, rdi, r8-r11
- **XMM registers**: xmm0-xmm15 (all volatile)

These are callee-saved (preserved across calls):
- **General purpose**: rbx, rbp, r12-r15
- **XMM registers**: None! All XMM registers are caller-saved

**Implication:** XMM registers are NEVER safe across function calls. Always use stack.

### Performance Considerations

Stack operations are fast (L1 cache) and the slight overhead is negligible compared to:
- The complexity of register liveness analysis
- The difficulty of debugging register corruption bugs
- The risk of subtle, hard-to-reproduce errors

**Premature optimization**: Trying to "optimize" by using registers for intermediate values often leads to bugs that cost far more time to debug than the microseconds saved.

### Code Patterns and Helpers

**Helper function for safe binary operations:**

```go
// Use this helper instead of manually managing registers:
func (fc *FlapCompiler) compileBinaryOpSafe(left, right Expression, operator string)
```

This helper encapsulates the stack-first pattern and should be used whenever possible.

**Comment template for manual implementations:**

When you must manually implement expression compilation with sub-expressions, use this comment pattern:

```go
// Compile left operand
fc.compileExpression(leftExpr)
// Save to stack (registers may be clobbered by sub-expression evaluation)
fc.out.SubImmFromReg("rsp", 16)
fc.out.MovXmmToMem("xmm0", "rsp", 0)
// Compile right operand (safe - can use any registers)
fc.compileExpression(rightExpr)
// Restore left operand from stack
fc.out.MovMemToXmm("xmm1", "rsp", 0)
fc.out.AddImmToReg("rsp", 16)
// Now xmm0 has right, xmm1 has left - ready to use
```

**Red flags to watch for:**

These patterns are potential bugs:
- `fc.out.MovRegToReg("xmm2", "xmm0")` followed by `fc.compileExpression(...)` - xmm2 will likely be clobbered
- Saving to XMM registers (xmm2-xmm15) across `fc.compileExpression()` calls
- Assuming any XMM register preserves its value across function calls
- Using XMM registers for "temporary" storage without checking call paths

**Safe patterns:**

These patterns are safe:
- Stack-based storage: `SubImmFromReg` → `MovXmmToMem` → ... → `MovMemToXmm` → `AddImmToReg`
- Using callee-saved general-purpose registers (rbx, r12-r15) but ONLY in functions you control the prologue/epilogue for
- Register-to-register moves within a single basic block with no function calls

## Nested Loop Implementation Design

**Problem:** Supporting arbitrary depth nested loops where each loop maintains its counter, limit, and iterator variable.

**Failed Approach:** Register-based storage using r12/r13
- Works for 2 levels but fails for 3+ because only saves the immediately outer loop's registers
- Push/pop pattern creates LIFO order: push A's registers → push B's registers → pop B's registers → pop A's registers
- Inner loop restore happens before outer loop completes

**Correct Solution:** Stack-based storage
```
Each loop allocates dedicated stack space (32 bytes, 16-byte aligned):
- [rsp + 0]:  counter (current iteration value)
- [rsp + 8]:  limit (end value)  
- [rsp + 16]: (padding for alignment)

Loop execution:
1. Allocate stack space: sub rsp, 32
2. Store counter/limit to stack
3. Load counter/limit to r12/r13 for loop condition checks
4. Update counter, store back to stack
5. Nested loops allocate their own stack slots
6. Deallocate on exit: add rsp, 32
```

**Key insight:** Each nested loop level has isolated stack slots, preventing interference.

**Files:** `parser.go:4419-4516` (compileRangeLoop function)

## Stack Alignment and Printf Bug

**Problem:** SIGBUS crashes when calling printf after nested loops.

**Root Cause #1:** Range loops allocated 24 bytes (not 16-byte aligned), violating x86-64 ABI requirement.

**Root Cause #2:** Printf had buggy alignment code:
```go
// WRONG - r10 is caller-saved, gets clobbered by printf
fc.out.MovRegToReg("r10", "rsp")
fc.out.AndImm("rsp", -16)
// ... call printf ...
fc.out.MovRegToReg("rsp", "r10")  // BROKEN: r10 was clobbered!
```

**Solution:**
1. Changed range loop allocation from 24 to 32 bytes (16-byte aligned)
2. Removed buggy printf alignment code - no longer needed since stack is always aligned

**Lesson:** Stack must be 16-byte aligned before any function call. Use proper multiples (16, 32, 48, ...) for stack allocations.

## Variable Scoping and Priority Order in Optimization Passes

When resolving variable/parameter references during optimization, the priority order is:

1. **Lambda parameters** - Highest priority, shadows all outer scopes
2. **Loop iterators** - Local to loop scope
3. **Local variables** - Variables defined in the current scope
4. **Outer scope variables** - Variables from enclosing scopes
5. **Constants** - Constant propagation applies last, only if not shadowed

### Constant Propagation and Lambda Scoping

**Critical Rule:** Lambda parameters must shadow outer variables during constant propagation.

**Bug Pattern:**
```flap
x := 10.5              // Outer variable marked as constant
square := x => x * x   // Lambda parameter 'x'
square(4.0)            // WRONG: returns 110.25 (10.5 * 10.5)
                       // RIGHT: should return 16 (4.0 * 4.0)
```

**Cause:** Constant propagation replaced lambda parameter `x` with outer constant `10.5`.

**Solution:** When propagating into lambda bodies, temporarily remove lambda parameters from the constants map:

```go
case *LambdaExpr:
    savedConstants := make(map[string]Expression)
    for _, param := range e.Params {
        if oldVal, existed := cp.constants[param]; existed {
            savedConstants[param] = oldVal
            delete(cp.constants, param)
        }
    }

    // Propagate into body with parameters shadowing outer constants
    if newBody, bodyChanged := cp.propagateInExpr(e.Body); bodyChanged {
        e.Body = newBody
    }

    // Restore outer constants
    for param, oldVal := range savedConstants {
        cp.constants[param] = oldVal
    }
```

### Mutation Tracking in Expressions

Constant propagation must detect mutations that occur within expressions, not just in assignment statements.

**Mutations can occur in:**
- Match expression branches: `n % 2 == 0 { -> n <- n / 2 }`
- Block expressions
- Lambda bodies
- Binary expressions with `<-` operator
- Postfix expressions: `steps++`

**Implementation:** Add `findMutationsInExprWithDepth()` that recursively searches expressions with depth limiting (max 100 levels) to prevent infinite recursion.

**Example requiring mutation tracking:**
```flap
n := 27
n % 2 == 0 {
    -> n <- n / 2      // Mutation in match branch
    ~> n <- (3*n) + 1  // Must be detected
}
```

### Dead Code Elimination Expression Handling

DCE must track variable usage in all expression types:

**Critical expression types:**
- `FStringExpr` - F-string interpolations: `f"Hello {name}"`
- `DirectCallExpr` - Direct function calls
- `NamespacedIdentExpr` - Dot notation: `data.field`
- `PostfixExpr` - Postfix operations: `i++`
- `VectorExpr` - Vector literals
- `LoopExpr` - Loop expressions
- `MultiLambdaExpr` - Pattern matching lambdas

**Bug Pattern:** Variable marked as unused and removed, causing "undefined variable" errors.

**Solution:** Add cases in `markUsedInExpr()` for all expression types that can reference variables.

### Loop Unrolling State Expression Handling

When unrolling loops with loop state expressions (`@i`, `@i1`, `@i2`):

**Loop Level Semantics:**
- `@i` (LoopLevel=0) - Current loop iterator
- `@i1` (LoopLevel=1) - Outermost loop iterator
- `@i2` (LoopLevel=2) - Second level loop iterator
- etc.

**Unrolling Rules:**
1. Only unroll loops with constant bounds and ≤ 8 iterations
2. Check if loop contains nested loops before substitution
3. When unrolling:
   - Replace `@i1` (LoopLevel=1) with iteration value
   - Decrement LoopLevel for `@i2+` (LoopLevel>1)
   - Only replace `@i` (LoopLevel=0) if no nested loops

**Example:**
```flap
@ i in 0..<3 {              // Outer loop
    @ j in 10..<12 {         // Inner loop
        printf("@i1=%v, @i2=%v, @i=%v", @i1, @i2, @i)
    }
}
```

After outer loop unrolls:
- `@i1` → 0, 1, 2 (replaced with values)
- `@i2` → `@i1` (LoopLevel decremented from 2 to 1)
- `@i` → `@i` (stays as-is, will be replaced when inner loop unrolls)

### Recursion Safety

All recursive AST traversals must include depth limiting to prevent stack overflow on malformed or adversarial input.

**Implementation Pattern:**
```go
const maxRecursionDepth = 100

func traverse(node Node, depth int) {
    if depth > maxRecursionDepth {
        return  // Or return error
    }
    // Process node...
    traverse(child, depth+1)
}
```

**Apply to:** findMutations, propagateInExpr, markUsedInExpr, and any other recursive AST traversal.

## macOS ARM64 Execution Issue (2025-10-26)

### Discovery
macOS ARM64 binaries generated by flapc are structurally valid Mach-O files but are killed with SIGKILL (exit code 137) before code execution. Even `codesign` reports "failed strict validation".

### Progress
1. Fixed LINKEDIT segment size calculation - was including 4KB code signature space when none was written
2. Added LC_CODE_SIGNATURE load command with 4KB reserved space (zeros)
3. Binary now has proper structure with filesize matching actual data

### Current Status
Binaries still fail with:
- `codesign`: "main executable failed strict validation"
- Execution: SIGKILL (137) before dyld loads any code
- Reason unknown - structure appears valid per `otool` and `pagestuff`

### Missing Load Commands vs Clang
Clang binaries have additional load commands we lack:
- LC_UUID
- LC_SOURCE_VERSION
- LC_FUNCTION_STARTS
- LC_DATA_IN_CODE
- LC_DYLD_EXPORTS_TRIE

One or more of these may be required for validation.

### Next Steps
1. Add LC_UUID (common requirement)
2. Generate proper ad-hoc code signature instead of zeros
3. Or investigate why validation fails even with reserved space
